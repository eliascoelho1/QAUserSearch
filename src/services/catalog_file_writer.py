"""CatalogFileWriter for generating YAML catalog files."""

from datetime import UTC, datetime
from pathlib import Path
from typing import Any

import structlog
import yaml

from src.schemas.catalog_yaml import (
    CatalogIndex,
    IndexEntry,
    SourceMetadataYaml,
)

logger = structlog.get_logger(__name__)


class ExtractionRollbackError(Exception):
    """Raised when an extraction fails and rollback is performed."""

    def __init__(self, message: str, original_error: Exception) -> None:
        """Initialize the rollback error.

        Args:
            message: Description of what happened.
            original_error: The exception that triggered the rollback.
        """
        super().__init__(message)
        self.original_error = original_error


class CatalogFileWriter:
    """Writer for generating and updating YAML catalog files.

    This class handles writing source metadata to YAML files and updating
    the global catalog index. It supports preserving manually edited fields
    (description, enrichment_status) during re-extraction.
    """

    # Fields that are manually editable and should be preserved during merge
    MANUAL_FIELDS = {"description", "enrichment_status"}

    # Header comment documenting editable fields
    YAML_HEADER = """# Catalog Source Metadata
#
# This file was auto-generated by the qa-catalog CLI.
# You can safely edit the following fields, which will be preserved on re-extraction:
#
#   - description: Add a human-readable description for any column
#   - enrichment_status: Set to "enriched" after adding descriptions
#
# Available enrichment_status values:
#   - not_enriched (default)
#   - pending_enrichment
#   - enriched
#
# Example:
#   columns:
#     - path: status
#       name: status
#       ...
#       description: "Status da fatura: OPEN=aberta, PAID=paga, OVERDUE=vencida"
#       enrichment_status: enriched
#

"""

    def __init__(self, catalog_path: Path | str) -> None:
        """Initialize the catalog file writer.

        Args:
            catalog_path: Path to the catalog directory.
        """
        self._catalog_path = Path(catalog_path)
        self._log = logger.bind(catalog_path=str(catalog_path))

    def write_source(
        self,
        source: SourceMetadataYaml,
        merge_manual_fields: bool = False,
    ) -> Path:
        """Write source metadata to a YAML file.

        Args:
            source: Source metadata to write.
            merge_manual_fields: If True, preserve description and
                enrichment_status from existing file.

        Returns:
            Path to the written YAML file.
        """
        log = self._log.bind(
            db_name=source.db_name,
            table_name=source.table_name,
            merge_manual_fields=merge_manual_fields,
        )
        log.info("Writing source to YAML file")

        # Ensure directory structure exists
        self._ensure_directories(source.db_name, source.table_name)

        # Prepare data for writing
        yaml_data = source.to_yaml_dict()

        # Merge manual fields from existing file if requested
        if merge_manual_fields:
            yaml_data = self._merge_manual_fields(
                source.db_name, source.table_name, yaml_data
            )

        # Write to file with header comment
        file_path = self._get_source_file_path(source.db_name, source.table_name)

        yaml_content = yaml.dump(
            yaml_data,
            default_flow_style=False,
            allow_unicode=True,
            sort_keys=False,
            indent=2,
        )

        with file_path.open("w", encoding="utf-8") as f:
            f.write(self.YAML_HEADER)
            f.write(yaml_content)

        log.info("Source written successfully", file_path=str(file_path))
        return file_path

    def update_index(self, source: SourceMetadataYaml) -> Path:
        """Update the catalog index with source information.

        If the source already exists in the index, its entry is updated.
        If the source is new, it is added to the index.

        Args:
            source: Source metadata that was written.

        Returns:
            Path to the index file.
        """
        log = self._log.bind(
            db_name=source.db_name,
            table_name=source.table_name,
        )
        log.info("Updating catalog index")

        index_path = self._catalog_path / "catalog.yaml"

        # Load existing index or create new one
        if index_path.exists():
            with index_path.open("r", encoding="utf-8") as f:
                index_data = yaml.safe_load(f)
            index = CatalogIndex.from_yaml_dict(index_data)
        else:
            index = CatalogIndex(
                version="1.0",
                generated_at=datetime.now(tz=UTC),
                sources=[],
            )

        # Create new entry for the source
        new_entry = IndexEntry(
            db_name=source.db_name,
            table_name=source.table_name,
            last_extracted=source.extracted_at,
            file_path=f"sources/{source.db_name}/{source.table_name}.yaml",
        )

        # Find and update or add the entry
        existing_entry = index.find_source(source.db_name, source.table_name)
        if existing_entry is not None:
            # Replace existing entry
            index.sources = [
                s
                for s in index.sources
                if not (
                    s.db_name == source.db_name and s.table_name == source.table_name
                )
            ]

        index.sources.append(new_entry)

        # Update generated_at timestamp
        index = CatalogIndex(
            version=index.version,
            generated_at=datetime.now(tz=UTC),
            sources=index.sources,
        )

        # Write updated index
        with index_path.open("w", encoding="utf-8") as f:
            yaml.dump(
                index.to_yaml_dict(),
                f,
                default_flow_style=False,
                allow_unicode=True,
                sort_keys=False,
                indent=2,
            )

        log.info("Index updated successfully", source_count=len(index.sources))
        return index_path

    def write_source_with_rollback(
        self,
        source: SourceMetadataYaml,
        merge_manual_fields: bool = False,
    ) -> Path:
        """Write source metadata to YAML with rollback on failure.

        This method creates backups of existing files before writing and
        restores them if any error occurs during the write or index update.

        Args:
            source: Source metadata to write.
            merge_manual_fields: If True, preserve description and
                enrichment_status from existing file.

        Returns:
            Path to the written YAML file.

        Raises:
            ExtractionRollbackError: If write fails and rollback is performed.
        """
        log = self._log.bind(
            db_name=source.db_name,
            table_name=source.table_name,
        )

        source_path = self._get_source_file_path(source.db_name, source.table_name)
        index_path = self._catalog_path / "catalog.yaml"

        # Create backups if files exist
        backup_paths: list[tuple[Path, Path]] = []  # (original, backup)

        try:
            if source_path.exists():
                source_backup = source_path.with_suffix(".yaml.bak")
                source_backup.write_bytes(source_path.read_bytes())
                backup_paths.append((source_path, source_backup))
                log.debug("Created source backup", backup=str(source_backup))

            if index_path.exists():
                index_backup = index_path.with_suffix(".yaml.bak")
                index_backup.write_bytes(index_path.read_bytes())
                backup_paths.append((index_path, index_backup))
                log.debug("Created index backup", backup=str(index_backup))

            # Perform the actual write operations
            result_path = self.write_source(source, merge_manual_fields)
            self.update_index(source)

            # Success - remove backups
            for _, backup in backup_paths:
                if backup.exists():
                    backup.unlink()
                    log.debug("Removed backup", backup=str(backup))

            return result_path

        except Exception as e:
            log.error("Write failed, performing rollback", error=str(e))

            # Rollback: restore backups
            for original, backup in backup_paths:
                if backup.exists():
                    backup.rename(original)
                    log.info("Restored from backup", file=str(original))

            # Remove any partially written new file that didn't have a backup
            if source_path.exists() and not any(
                orig == source_path for orig, _ in backup_paths
            ):
                source_path.unlink()
                log.info("Removed partially written file", file=str(source_path))

            raise ExtractionRollbackError(
                f"Failed to write {source.db_name}.{source.table_name}: {e}",
                original_error=e,
            ) from e

    def _merge_manual_fields(
        self,
        db_name: str,
        table_name: str,
        new_data: dict[str, Any],
    ) -> dict[str, Any]:
        """Merge manual fields from existing file into new data.

        This preserves 'description' and 'enrichment_status' fields that
        may have been manually edited by QAs.

        Args:
            db_name: Database name.
            table_name: Table name.
            new_data: New data to be written.

        Returns:
            Merged data with manual fields preserved.
        """
        log = self._log.bind(db_name=db_name, table_name=table_name)

        file_path = self._get_source_file_path(db_name, table_name)

        if not file_path.exists():
            log.debug("No existing file to merge from")
            return new_data

        try:
            with file_path.open("r", encoding="utf-8") as f:
                existing_data = yaml.safe_load(f)
        except yaml.YAMLError as e:
            log.warning("Failed to parse existing file for merge", error=str(e))
            return new_data

        if not existing_data or "columns" not in existing_data:
            return new_data

        # Build lookup of existing columns by path
        existing_columns: dict[str, dict[str, Any]] = {
            col["path"]: col for col in existing_data.get("columns", [])
        }

        # Merge manual fields into new columns
        for new_col in new_data.get("columns", []):
            col_path = new_col["path"]
            if col_path in existing_columns:
                existing_col = existing_columns[col_path]
                for field in self.MANUAL_FIELDS:
                    if field in existing_col and existing_col[field] is not None:
                        new_col[field] = existing_col[field]
                        log.debug(
                            "Preserved manual field",
                            column=col_path,
                            field=field,
                            value=existing_col[field],
                        )

        return new_data

    def _ensure_directories(
        self,
        db_name: str,
        table_name: str,  # noqa: ARG002
    ) -> None:
        """Ensure the directory structure exists for a source.

        Args:
            db_name: Database name.
            table_name: Table name (unused but kept for consistency).
        """
        source_dir = self._catalog_path / "sources" / db_name
        source_dir.mkdir(parents=True, exist_ok=True)

    def _get_source_file_path(self, db_name: str, table_name: str) -> Path:
        """Get the file path for a source YAML file.

        Args:
            db_name: Database name.
            table_name: Table name.

        Returns:
            Path to the source YAML file.
        """
        return self._catalog_path / "sources" / db_name / f"{table_name}.yaml"
